apiVersion: apps/v1
kind: Deployment
metadata:
  name: ollama
  namespace: ollama
  labels:
    app: ollama
spec:
  selector:
    matchLabels:
      app: ollama
  template:
    metadata:
      labels:
        app: ollama
    spec:
      runtimeClassName: nvidia
      volumes:
        - name: ollama-models-volume
          persistentVolumeClaim:
            claimName: ollama-models
        - name: init-script
          configMap:
            name: ollama-init-script
            defaultMode: 0755
        - name: sd-models-volume
          persistentVolumeClaim:
            claimName: sd-models
        - name: generated-images
          persistentVolumeClaim:
            claimName: generated-images
        - name: code-base
          persistentVolumeClaim:
            claimName: code-base
        - name: llama-cpp
          hostPath:
            path: /llama.cpp
            type: DirectoryOrCreate
        - name: models
          hostPath:
            path: /models
            type: DirectoryOrCreate
      containers:
        - name: ollama
          image: 192.168.0.71:31500/inference:latest
          imagePullPolicy: Always
          command: ["/bin/sh"]
          args:
            - "-c"
            - |
              # Check if app.py exists in the mounted directory
              if [ -f /app/app.py ]; then
                echo "Found app.py in mounted volume, starting with live-reload"
                # Start Ollama and then the API with reload for live updates
                ollama serve &
                cd /app && ls -la && uvicorn app:app --host 0.0.0.0 --port 8000 --reload
              else
                echo "ERROR: app.py not found in mounted directory. Please ensure your code is correctly mounted."
                ls -la /app
                echo "Contents of parent directory:"
                ls -la /
                exit 1
              fi
          ports:
            - name: ollama
              containerPort: 11434
              protocol: TCP
            - name: sd
              containerPort: 8000
              protocol: TCP
          resources:
            requests:
              memory: "24Gi"
              cpu: "16"
              nvidia.com/gpu: "3"
            limits:
              memory: "30Gi"
              cpu: "20"
              nvidia.com/gpu: "3"
          volumeMounts:
            - name: ollama-models-volume
              mountPath: /root/.ollama
            - name: sd-models-volume
              mountPath: /root/.cache/huggingface/hub
            - name: init-script
              mountPath: /scripts
            - name: generated-images
              mountPath: /root/images
            - name: code-base
              mountPath: /app
            - name: llama-cpp
              mountPath: /llama.cpp
            - name: models
              mountPath: /models
          env:
            - name: OLLAMA_HOST
              value: "0.0.0.0"
            - name: OLLAMA_DEBUG
              value: "false"
            - name: PYTHONPATH
              value: "/app"
            - name: HF_TOKEN
              valueFrom:
                secretKeyRef:
                  name: hf-token
                  key: token
            # Memory management settings for Stable Diffusion
            # - name: SD_MAX_WIDTH
            #   value: "768"
            # - name: SD_MAX_HEIGHT
            #   value: "768"
            # - name: SD_DEFAULT_STEPS
            #   value: "30"
            # - name: USE_FP32_FOR_TEXT_ENCODERS
            #   value: "true"
            # - name: BNB_CUDA_VERSION
            #   value: "128"
            # rabbitmq settings
            - name: RABBITMQ_HOST
              value: "rabbitmq-0.rabbitmq.rabbitmq.svc.cluster.local"
            - name: RABBITMQ_PORT
              value: "5672"
            - name: RABBITMQ_USER
              value: "lsm"
            - name: RABBITMQ_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: rabbitmq
                  key: password
            - name: RABBITMQ_VHOST
              value: "/"
            # Internal service settings
            - name: MAISTRO_INTERNAL_API_KEY
              valueFrom:
                secretKeyRef:
                  name: internal-api-key
                  key: api_key
            - name: MAISTRO_BASE_URL
              value: "http://maistro.maistro.svc.cluster.local:8080"
            # - name: CUDA_HOME
            #   value: "/usr/local/cuda"
            # - name: LLAMA_CUBLAS
            #   value: "1"
            # - name: CMAKE_ARGS
            #   value: "-DLLAMA_CUBLAS=on"
            # - name: FORCE_CMAKE
            #   value: "1"
  strategy:
    type: Recreate
