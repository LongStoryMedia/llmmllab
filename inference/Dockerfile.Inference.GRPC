# Use the base image from the existing Inference service
FROM ghcr.io/llm-ml-lab/inference-base:latest

WORKDIR /app

# Install gRPC-specific dependencies
RUN pip install grpcio grpcio-tools grpcio-reflection protobuf

# Copy proto files
COPY proto/ /app/proto/

# Copy server code
COPY inference/grpc_server/ /app/grpc_server/

# Copy config files
COPY inference/config/ /app/config/

# Generate proto code
RUN cd /app/grpc_server/proto && python generate_proto.py

# Copy rest of the application
COPY inference/ /app/

# Expose gRPC port
EXPOSE 50051

# Set environment variables
ENV PYTHONPATH=/app
ENV GRPC_PORT=50051
ENV GRPC_MAX_WORKERS=10
ENV GRPC_MAX_MESSAGE_SIZE=104857600
ENV GRPC_MAX_CONCURRENT_RPCS=100
ENV GRPC_ENABLE_REFLECTION=true
ENV GRPC_REQUIRE_API_KEY=false
ENV GRPC_USE_TLS=false

# Run the server
CMD ["python", "/app/grpc_server/server.py"]
