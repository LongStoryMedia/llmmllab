"""
Inference engine for benchmarks.
Provides a way to interact with model inference backends.
"""

import datetime
from typing import Dict, Any, Optional
import logging
import random
import importlib.util

# Initialize module-level variables
PIPELINES_AVAILABLE = False
pipeline_factory = None
Message = None
MessageContent = None
MessageRole = None
ChatReq = None
ModelParameters = None
MessageContentType = None

# Check if required modules are available
try:
    # Check if modules exist before importing
    if (
        importlib.util.find_spec("pipelines") is not None
        and importlib.util.find_spec("models") is not None
    ):

        # Try to import the modules
        from runner.pipelines.factory import pipeline_factory
        from models import MessageContent, Message, MessageRole, ChatReq
        from models import ModelParameters, MessageContentType

        PIPELINES_AVAILABLE = True
    else:
        logging.warning(
            "Pipeline factory modules not found. Using mock inference for testing."
        )
except ImportError:
    logging.warning("Pipeline factory modules found but could not be imported.")


class InferenceEngine:
    """Handles model inference for benchmarks."""

    def __init__(self):
        """Initialize the inference engine."""
        self.logger = logging.getLogger("evaluation.utils.inference")

    def _mock_inference(self, prompt: str, max_tokens: int = 100) -> Dict[str, Any]:
        """
        Generate a mock inference response for testing.

        Args:
            prompt: The input prompt
            max_tokens: Maximum tokens to generate (ignored in mock)

        Returns:
            Dict with mock response and metrics
        """
        # Mock responses for code generation
        if "def " in prompt and ">>>" in prompt:
            # This appears to be a code challenge, generate a simple function
            fn_name = prompt.split("def ")[1].split("(")[0].strip()
            mock_response = f"""def {fn_name}(*args, **kwargs):
    # Implementation for {fn_name}
    result = None
    
    # Process the inputs
    if args and len(args) > 0:
        result = args[0]
    
    return result"""
        else:
            # Generic mock response
            mock_response = "This is a mock response for testing. In production, this would be generated by the actual model."

        # Mock metrics
        mock_metrics = {
            "response": mock_response,
            "total_duration": random.uniform(0.1, 2.0),
            "load_duration": random.uniform(0.05, 0.5),
            "eval_count": random.randint(10, 100),
        }

        return mock_metrics

    def run_single_inference(
        self,
        model_id: str,
        prompt: str,
        max_tokens: int = 10000,
        temperature: float = 0.7,
    ) -> Dict[str, Any]:
        """
        Run inference using the pipeline factory or fallback to mock inference for testing.

        Args:
            model_id: The model identifier
            prompt: The input prompt
            max_tokens: Maximum tokens to generate
            temperature: Sampling temperature

        Returns:
            Dict with response and metrics
        """
        # If pipelines aren't available or any required class is None, use mock inference
        if (
            not PIPELINES_AVAILABLE
            or Message is None
            or MessageContent is None
            or MessageRole is None
            or ChatReq is None
            or ModelParameters is None
            or MessageContentType is None
            or pipeline_factory is None
        ):

            self.logger.warning(f"Using mock inference for model: {model_id}")
            return self._mock_inference(prompt, max_tokens)

        try:
            # Now we know these classes are available, create message for inference
            message = Message(
                role=MessageRole.USER,
                content=[MessageContent(type=MessageContentType.TEXT, text=prompt)],
                id=999,
                conversation_id=999,
                created_at=datetime.datetime.now(tz=datetime.timezone.utc),
            )

            # Create request
            req = ChatReq(
                messages=[message],
                model=model_id,
                stream=True,
                think=False,
                options=ModelParameters(
                    temperature=temperature,
                    top_p=0.95,
                    top_k=40,
                    num_predict=max_tokens,
                ),
            )

            # Get pipeline and run inference
            pipe, load_time = pipeline_factory.get_pipeline(model_id)
            result_generator = pipe.run(req, load_time)

            # Process results
            full_response = ""
            final_metrics = {}

            for response in result_generator:
                if response.message.content and len(response.message.content) > 0:
                    full_response += response.message.content[0].text or ""

                if response.done:
                    final_metrics = {
                        "total_duration": getattr(response, "total_duration", 0),
                        "load_duration": getattr(response, "load_duration", 0),
                        "eval_count": getattr(response, "eval_count", 0),
                    }
                    break

            # Add response to metrics
            final_metrics["response"] = full_response
            return final_metrics

        except Exception as e:
            self.logger.error(f"Error running inference: {str(e)}")
            return self._mock_inference(prompt, max_tokens)
